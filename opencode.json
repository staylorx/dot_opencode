{
  "$schema": "https://opencode.ai/config.json",
  "plugin": [
    "@plannotator/opencode@latest",
    "@spoons-and-mirrors/subtask2@latest",
    "oh-my-opencode@latest"
  ],
  "mcp": {
    "context7": {
      "command": [
        "npx",
        "-y",
        "@upstash/context7-mcp"
      ],
      "enabled": true,
      "type": "local"
    },
    "local-files": {
      "command": [
        "npx",
        "-y",
        "localfiles-org"
      ],
      "enabled": true,
      "type": "local"
    },
    "sequential-thinking": {
      "command": [
        "npx",
        "-y",
        "@modelcontextprotocol/server-sequential-thinking"
      ],
      "enabled": true,
      "type": "local"
    }
  },
  "provider": {
    "ollama": {
      "models": {
        "MFDoom/deepseek-r1-tool-calling:32b": {
          "_launch": true,
          "name": "MFDoom/deepseek-r1-tool-calling:32b"
        },
        "glm-4.7-flash": {
          "_launch": true,
          "name": "glm-4.7-flash"
        },
        "qwen3-coder": {
          "_launch": true,
          "name": "qwen3-coder"
        },
        "qwen3:8b": {
          "_launch": true,
          "name": "qwen3:8b"
        },
        "qwen3:8b-q4_K_M": {
          "_launch": true,
          "name": "qwen3:8b-q4_K_M"
        }
      },
      "name": "Ollama (local)",
      "npm": "@ai-sdk/openai-compatible",
      "options": {
        "baseURL": "http://127.0.0.1:11434/v1"
      }
    }
  }
}
